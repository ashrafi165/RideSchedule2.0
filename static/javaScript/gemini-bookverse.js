// --- START OF FILE gemini2.js ---

import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from "https://cdn.jsdelivr.net/npm/@google/generative-ai/+esm";

// --- Configuration ---
// WARNING: Storing API keys client-side is insecure for production. Use a backend proxy.
const API_KEY = "AIzaSyCzDw4jYsl91ncl2wadhh9jpWdFmVtD0pg";
// const API_KEY = "AIzaSyCtAUsfYtkvHS69pPmefA_54lseRh276MI";
// const API_KEY = "AIzaSyACrU66ejkJV4-K2cWoH9tEH1W8lcEKFx8";
const MODEL_NAME = "gemini-1.5-pro-latest";

// --- System Prompt for Guiding the AI ---
const systemPrompt = `
You are TravelCostBot, a friendly assistant specialized in estimating travel costs within Bangladesh. Follow these rules for every user query:

1. Understand the Query
    The user will say something like:
    â€œI am at [Origin] and I want to go to [Destination]. How much money do I need?â€
    Both origin and destination will be places or landmarks within Bangladesh (e.g., Farmgate, Mirpurâ€¯1, Gulshanâ€¯2, Coxâ€™s Bazar).
2. Determine the Distance
    Look up or estimate the driving distance between Origin and Destination in kilometers.
    If you have an exact value, use that.
    If only a range is available (e.g., 15â€“16â€¯km), use that instead.
3. Compute the Fare
    Rate: 20 taka per kilometer
    If exact distance:
    Total Cost = Distance Ã— 20 taka, round to the nearest taka.
    If distance range:
    Min Cost = Lower bound Ã— 20 taka
    Max Cost = Upper bound Ã— 20 taka
    Round both.
4. Format Your Reply
    Tone: Friendly and conversational.
    Include:
    Restate the trip (e.g., â€œFrom Farmgate to Mirpurâ€¯1â€¦â€)
    Show the distance (exact or range)
    Show the calculation
    Exact:
    15â€¯km Ã— 20â€¯taka = 300â€¯taka
    Range:
    15â€¯km Ã— 20 = 300â€¯taka
    16â€¯km Ã— 20 = 320â€¯taka
    Give the final answer
    Exact: â€œYouâ€™ll need about 300â€¯taka.â€
    Range: â€œYouâ€™ll need between 300 and 320â€¯taka.â€
    End with a polite sign-off or follow-up prompt
    (e.g., â€œLet me know if you want directions!â€)
5. Example Responses
âž¤ Exact Distance Example
User: Iâ€™m at Farmgate and I want to go to Dhanmondi. How much money do I need?

Assistant:  
â€¢ Distance: 6â€¯km  
â€¢ Cost: 6â€¯km Ã— 20â€¯taka = 120â€¯taka  
Youâ€™ll need about 120â€¯taka to travel from Farmgate to Dhanmondi. ðŸ˜Š
âž¤ Range Example
User: Iâ€™m at Farmgate and I want to go to Mirpurâ€¯1. The distance is about 15â€“16â€¯km. How much money do I need?

Assistant:  
â€¢ Distance: 15â€“16â€¯km  
â€¢ Cost:  
   15â€¯km Ã— 20â€¯taka = 300â€¯taka  
   16â€¯km Ã— 20â€¯taka = 320â€¯taka  
Youâ€™ll need between 300 and 320â€¯taka to travel from Farmgate to Mirpurâ€¯1. ðŸ˜Š
Always doubleâ€‘check distances, do the math correctly, and keep your responses warm and helpful!
`;


// --- Conversation History (Optional for this specific task, but kept for structure) ---
// Note: For this focused task, we might not strictly need history,
// as the system prompt guides each interaction independently.
let chatHistory = [];
console.log("Initial chat history:", chatHistory);

// --- Initialize Gemini ---
let genAI;
let model;
try {
    genAI = new GoogleGenerativeAI(API_KEY);
    model = genAI.getGenerativeModel({ model: MODEL_NAME });
    console.log("GoogleGenerativeAI initialized successfully.");
} catch (error) {
    console.error("Error initializing GoogleGenerativeAI:", error);
    alert("Failed to initialize the AI service. Please check the API key and configuration in gemini2.js");
}

// --- Safety Settings ---
// Keeping safety settings minimal or moderate is usually fine for this use case.
// You can uncomment and adjust if needed, but BLOCK_NONE is generally not recommended.
const safetySettings = [
    { category: HarmCategory.HARM_CATEGORY_HARASSMENT,         threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
    { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,        threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
    { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
    { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,  threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE },
];
// const safetySettings = []; // Use with caution if specific content needs are understood


// --- Generation Configuration ---
const generationConfig = {
    temperature: 0.6, // Increased slightly for more natural language (adjust 0.5-0.7 as needed)
    topK: 1,
    topP: 0.95,
    maxOutputTokens: 256, // Increased slightly to allow for explanations
};




/**
 * Sends the system prompt + new user prompt to the Gemini API
 * and streams the response (expected to be category names or refusal).
 * Updates history upon successful completion.
 *
 * @param {string} newPrompt - The user's latest message (their interests).
 * @param {HTMLElement} targetElement - The message element to update.
 * @param {object} callbacks - UI update callback functions.
 */
async function streamChatResponse(newPrompt, targetElement, callbacks) {
    if (!model) {
        const errorMsg = "AI Model is not initialized. Check API Key and console.";
        console.error(errorMsg);
        callbacks.onError(targetElement, errorMsg);
        callbacks.onComplete(targetElement); // Still complete UI flow
        return;
    }

    let fullResponse = "";
    let isFirstChunk = true;
    let streamHasData = false;

    // *** MODIFIED: Construct the input with the System Prompt ***
    // We combine the system instructions and the user's current query into one user turn.
    // For this task, sending the whole chatHistory might confuse the model,
    // so we send the instructions + current query each time.
    const messagesToSend = [
        {
            role: "user",
            parts: [{ text: systemPrompt + "\n\nUser's Interests:\n" + newPrompt }]
            // Note: We are NOT including the potentially long chatHistory here.
            // The systemPrompt provides all necessary context for *this specific task*.
        }
    ];

    console.log("Sending to Gemini (including system prompt):\n", messagesToSend[0].parts[0].text.substring(0, 500) + "..."); // Log start of prompt

    let result;
    try {
        console.log("Calling model.generateContentStream()...");
        result = await model.generateContentStream({
            contents: messagesToSend,
            generationConfig,
            safetySettings,
        });
        console.log("model.generateContentStream() call returned. Awaiting stream...");

        for await (const chunk of result.stream) {
            // console.log("Received stream chunk:", chunk); // Debugging

            // Immediate check for prompt feedback in the chunk itself (less common now)
            if (chunk.promptFeedback && chunk.promptFeedback.blockReason) {
                const blockMessage = `Blocked based on prompt (during stream): ${chunk.promptFeedback.blockReason}`;
                console.error(blockMessage);
                throw new Error(blockMessage);
            }

            const chunkText = chunk.text();
            // console.log("Extracted chunk text:", chunkText); // Debugging

            if (chunkText !== undefined && chunkText !== null) {
                if (chunkText.length > 0) {
                    streamHasData = true;
                    fullResponse += chunkText;
                    // **MODIFICATION**: Don't parse with Marked for this specific output.
                    // We expect plain text (category names or refusal message).
                    // If you *need* markdown elsewhere, conditionally apply it based on context.
                    // const parsedHtml = typeof marked !== 'undefined' ? marked.parse(fullResponse) : fullResponse;
                    const parsedHtml = typeof marked !== 'undefined' ? marked.parse(fullResponse) : fullResponse;
                    callbacks.onData(targetElement, parsedHtml, isFirstChunk);
                    isFirstChunk = false;
                }
            }
        }

        console.log("Finished iterating through stream.");

        // Check the final aggregated response object
        const finalResponse = await result.response;
        console.log("Final response object:", JSON.stringify(finalResponse, null, 2));

        const promptFeedback = finalResponse?.promptFeedback;
        const finishReason = finalResponse?.candidates?.[0]?.finishReason;
        const safetyRatings = finalResponse?.candidates?.[0]?.safetyRatings;

        // Check for blocks or unexpected finish reasons in the final response
        if (promptFeedback?.blockReason) {
            const blockMessage = `Request blocked by API. Reason: ${promptFeedback.blockReason}`;
            console.error(blockMessage);
            throw new Error(blockMessage);
        }

        if (finishReason && finishReason !== "STOP" && finishReason !== "MAX_TOKENS") {
             const reasonMessage = `Stream finished unexpectedly. Reason: ${finishReason}`;
             console.warn(reasonMessage, "Safety Ratings:", safetyRatings);
             if (finishReason === "SAFETY") {
                throw new Error("Response blocked by safety filters.");
             } else {
                 throw new Error(reasonMessage); // Other non-STOP reasons are errors here
             }
        }

        // Final check: Did we get any actual content?
        if (streamHasData) {
            console.log("Stream finished successfully with data. Updating history.");
            // Update history (optional, but can be useful for debugging UI)
            chatHistory.push({ role: "user", parts: [{ text: newPrompt }] }); // Store original user prompt
            chatHistory.push({ role: "model", parts: [{ text: fullResponse }] }); // Store AI response
            console.log("Updated History:", JSON.stringify(chatHistory.slice(-4))); // Log recent history
            callbacks.onComplete(targetElement);
        } else {
            // No stream data received, even if finishReason was STOP
            console.warn("Stream finished, but no text content was processed/received.");
             if (finishReason === "STOP") {
                 // This implies the model chose to generate nothing based on the prompt.
                 // Could be a refusal, or it genuinely couldn't find a category.
                 // Let's return a clearer message instead of a generic error.
                 callbacks.onError(targetElement, "The AI could not determine a suitable category or chose not to respond based on the input.");
                 callbacks.onComplete(targetElement); // Still complete UI flow
             } else {
                 // If reason wasn't STOP and no data, likely an underlying issue.
                 throw new Error("Received an empty response from the AI. (Reason Unknown)");
             }
        }

    } catch (error) {
        console.error("Error during Gemini stream processing:", error);
        callbacks.onError(targetElement, error.message || "An unknown error occurred while contacting the AI.");
        callbacks.onComplete(targetElement); // Ensure UI is re-enabled
    }
}

// Ensure the function is globally accessible for your other scripts (like scripts.js)
window.streamChatResponse = streamChatResponse;
console.log("streamChatResponse available on window?", typeof window.streamChatResponse === 'function');

// --- END OF FILE gemini2.js ---